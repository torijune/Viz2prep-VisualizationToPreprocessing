#!/usr/bin/env python3
"""
ë°ì´í„° ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ë©”ì¸ ì‹¤í–‰ íŒŒì¼
ì‚¬ìš©ë²•: 
  python main.py --dataset datasets/Iris/Iris.csv
  python main.py --X datasets/Iris/X.csv --Y datasets/Iris/Y.csv
"""

import os
import sys
import argparse
import pandas as pd
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from workflow_graph import build_specialized_workflow
from workflow_state import WorkflowState

def load_dataset(dataset_path: str) -> tuple:
    """
    ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  X, Yë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        dataset_path (str): ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        tuple: (X_df, Y_df) íŠ¹ì„± ë³€ìˆ˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ë°ì´í„°í”„ë ˆì„
        
    Raises:
        FileNotFoundError: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        Exception: ê¸°íƒ€ ë¡œë“œ ì˜¤ë¥˜
    """
    try:
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_extension = Path(dataset_path).suffix.lower()
        
        # í™•ì¥ìì— ë”°ë¥¸ ë¡œë“œ ë°©ë²• ê²°ì •
        if file_extension == '.csv':
            df = pd.read_csv(dataset_path)
        elif file_extension in ['.xlsx', '.xls']:
            df = pd.read_excel(dataset_path)
        elif file_extension == '.json':
            df = pd.read_json(dataset_path)
        elif file_extension == '.parquet':
            df = pd.read_parquet(dataset_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
        
        print(f"âœ… [LOAD] ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {df.shape}")
        print(f"ğŸ“Š [LOAD] ì»¬ëŸ¼: {list(df.columns)}")
        print(f"ğŸ“Š [LOAD] ë°ì´í„° íƒ€ì…: {df.dtypes.to_dict()}")
        print(f"ğŸ“Š [LOAD] ê²°ì¸¡ê°’: {df.isnull().sum().sum()}ê°œ")
        
        # X, Y ë¶„ë¦¬
        X, Y = separate_features_and_target(df, dataset_path)
        
        return X, Y
        
    except FileNotFoundError as e:
        print(f"âŒ [ERROR] {e}")
        return None, None
    except Exception as e:
        print(f"âŒ [ERROR] ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None, None

def load_separate_datasets(X_path: str, Y_path: str) -> tuple:
    """
    Xì™€ Yë¥¼ ë³„ë„ì˜ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        X_path (str): X ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ
        Y_path (str): Y ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        tuple: (X_df, Y_df) íŠ¹ì„± ë³€ìˆ˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ë°ì´í„°í”„ë ˆì„
    """
    try:
        # X ë°ì´í„° ë¡œë“œ
        if not os.path.exists(X_path):
            raise FileNotFoundError(f"X ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {X_path}")
        
        X_extension = Path(X_path).suffix.lower()
        if X_extension == '.csv':
            X = pd.read_csv(X_path)
        elif X_extension in ['.xlsx', '.xls']:
            X = pd.read_excel(X_path)
        elif X_extension == '.json':
            X = pd.read_json(X_path)
        elif X_extension == '.parquet':
            X = pd.read_parquet(X_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” X íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {X_extension}")
        
        # Y ë°ì´í„° ë¡œë“œ
        if not os.path.exists(Y_path):
            raise FileNotFoundError(f"Y ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Y_path}")
        
        Y_extension = Path(Y_path).suffix.lower()
        if Y_extension == '.csv':
            Y = pd.read_csv(Y_path)
        elif Y_extension in ['.xlsx', '.xls']:
            Y = pd.read_excel(Y_path)
        elif Y_extension == '.json':
            Y = pd.read_json(Y_path)
        elif Y_extension == '.parquet':
            Y = pd.read_parquet(Y_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Y íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {Y_extension}")
        
        print(f"âœ… [LOAD] X ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {X.shape}")
        print(f"ğŸ“Š [LOAD] X ì»¬ëŸ¼: {list(X.columns)}")
        print(f"ğŸ“Š [LOAD] X ë°ì´í„° íƒ€ì…: {X.dtypes.to_dict()}")
        print(f"ğŸ“Š [LOAD] X ê²°ì¸¡ê°’: {X.isnull().sum().sum()}ê°œ")
        
        print(f"âœ… [LOAD] Y ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {Y.shape}")
        print(f"ğŸ“Š [LOAD] Y ì»¬ëŸ¼: {list(Y.columns)}")
        print(f"ğŸ“Š [LOAD] Y ë°ì´í„° íƒ€ì…: {Y.dtypes.to_dict()}")
        print(f"ğŸ“Š [LOAD] Y ê²°ì¸¡ê°’: {Y.isnull().sum().sum()}ê°œ")
        
        return X, Y
        
    except FileNotFoundError as e:
        print(f"âŒ [ERROR] {e}")
        return None, None
    except Exception as e:
        print(f"âŒ [ERROR] ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None, None

def separate_features_and_target(df: pd.DataFrame, dataset_path: str = None) -> tuple:
    """
    ë°ì´í„°í”„ë ˆì„ì„ íŠ¹ì„± ë³€ìˆ˜(X)ì™€ íƒ€ê²Ÿ ë³€ìˆ˜(Y)ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        dataset_path (str): ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        
    Returns:
        tuple: (X_df, Y_df) íŠ¹ì„± ë³€ìˆ˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ë°ì´í„°í”„ë ˆì„
    """
    # ì¼ë°˜ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ì»¬ëŸ¼ëª…ë“¤
    target_columns = [
        'target', 'target_variable', 'label', 'class', 'y', 'Y',
        'income', 'salary', 'price', 'value', 'result', 'outcome',
        'survived', 'survival', 'death', 'alive',
        'default', 'churn', 'fraud', 'spam',
        'diagnosis', 'disease', 'cancer', 'malignant',
        'species', 'type', 'category'
    ]
    
    # ë°ì´í„°ì…‹ë³„ íŠ¹ìˆ˜í•œ íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬
    dataset_specific_targets = {
        'adult': ['income'],
        'iris': ['species'],
        'titanic': ['survived'],
        'breast_cancer': ['diagnosis'],
        'diabetes': ['outcome'],
        'heart': ['target'],
        'wine': ['target'],
        'digits': ['target']
    }
    
    # íŒŒì¼ëª…ì—ì„œ ë°ì´í„°ì…‹ íƒ€ì… ì¶”ì¸¡
    dataset_type = None
    if dataset_path:
        file_name = Path(dataset_path).stem.lower()
        for key in dataset_specific_targets.keys():
            if key in file_name:
                dataset_type = key
                break
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì°¾ê¸°
    target_col = None
    
    # 1. ë°ì´í„°ì…‹ë³„ íŠ¹ìˆ˜í•œ íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
    if dataset_type and dataset_type in dataset_specific_targets:
        for col in dataset_specific_targets[dataset_type]:
            if col in df.columns:
                target_col = col
                break
    
    # 2. ì¼ë°˜ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ëª… í™•ì¸
    if target_col is None:
        for col in target_columns:
            if col in df.columns:
                target_col = col
                break
    
    # 3. ë§ˆì§€ë§‰ ì»¬ëŸ¼ì„ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì‚¬ìš© (ê¸°ë³¸ê°’)
    if target_col is None:
        target_col = df.columns[-1]
        print(f"âš ï¸  [WARNING] ëª…ì‹œì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë§ˆì§€ë§‰ ì»¬ëŸ¼ '{target_col}'ì„ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # X, Y ë¶„ë¦¬
    Y = df[[target_col]].copy()
    X = df.drop(columns=[target_col]).copy()
    
    print(f"ğŸ“Š [SEPARATE] X shape: {X.shape}, Y shape: {Y.shape}")
    print(f"ğŸ“Š [SEPARATE] íŠ¹ì„± ë³€ìˆ˜: {list(X.columns)}")
    print(f"ğŸ“Š [SEPARATE] íƒ€ê²Ÿ ë³€ìˆ˜: {list(Y.columns)}")
    
    return X, Y

def run_workflow(dataset_path: str = None, X_path: str = None, Y_path: str = None, query: str = None) -> dict:
    """
    ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        dataset_path (str): í†µí•© ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        X_path (str): X ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        Y_path (str): Y ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        query (str): ì‚¬ìš©ì ì¿¼ë¦¬ (ê¸°ë³¸ê°’: ìë™ ìƒì„±)
        
    Returns:
        dict: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼
    """
    print("=" * 80)
    print("ğŸš€ ë°ì´í„° ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    print("=" * 80)
    
    # 1. ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_path:
        # í†µí•© ë°ì´í„°ì…‹ì—ì„œ X, Y ë¶„ë¦¬
        X, Y = load_dataset(dataset_path)
        if X is None or Y is None:
            return {"error": "ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨"}
    elif X_path and Y_path:
        # ë³„ë„ì˜ X, Y íŒŒì¼ì—ì„œ ë¡œë“œ
        X, Y = load_separate_datasets(X_path, Y_path)
        if X is None or Y is None:
            return {"error": "X ë˜ëŠ” Y ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨"}
    else:
        return {"error": "ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. --dataset ë˜ëŠ” --Xì™€ --Yë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."}
    
    # 2. ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„± (ì‚¬ìš©ìê°€ ì œê³µí•˜ì§€ ì•Šì€ ê²½ìš°)
    if query is None:
        # ë°ì´í„°ì…‹ íŠ¹ì„±ì— ë”°ë¥¸ ìë™ ì¿¼ë¦¬ ìƒì„±
        X_numeric_cols = X.select_dtypes(include=['number']).columns.tolist()
        X_categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
        
        query_parts = []
        if X_numeric_cols:
            query_parts.append(f"ìˆ˜ì¹˜í˜• ë³€ìˆ˜({', '.join(X_numeric_cols)}) ì •ê·œí™”")
        if X_categorical_cols:
            query_parts.append(f"ë²”ì£¼í˜• ë³€ìˆ˜({', '.join(X_categorical_cols)}) ì›í•« ì¸ì½”ë”©")
        
        query = f"ì´ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì— ì í•©í•˜ë„ë¡ ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”. {'í•˜ê³  '.join(query_parts)}ë¥¼ ì ìš©í•´ì£¼ì„¸ìš”."
    
    print(f"ğŸ“ [QUERY] ì‚¬ìš©ì ìš”ì²­: {query}")
    
    # 3. ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶•
    try:
        workflow = build_specialized_workflow()
        print("âœ… [GRAPH] ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ [ERROR] ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        return {"error": f"ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶• ì‹¤íŒ¨: {e}"}
    
    # 4. ì´ˆê¸° ìƒíƒœ ì„¤ì • (X, Y ë¶„ë¦¬)
    initial_state = {
        "query": query,
        "X": X,
        "Y": Y
    }
    
    # 5. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    print("\nğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...")
    print("=" * 50)
    
    try:
        result = workflow.invoke(initial_state)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!")
        print("=" * 80)
        
        # 6. ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ ìµœì¢… ì‘ë‹µ:")
        print(result.get("final_response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
        
        # ì „ì²˜ë¦¬ ìš”ì•½ ì¶œë ¥
        summary = result.get("preprocessing_summary", {})
        if summary:
            print("\nğŸ“Š ì „ì²˜ë¦¬ ìš”ì•½:")
            processing_steps = summary.get('processing_steps', {})
            if processing_steps:
                print(f"   - ì„±ê³µë¥ : {processing_steps.get('success_rate', 0):.1f}%")
                print(f"   - ì„±ê³µí•œ ë‹¨ê³„: {processing_steps.get('successful_steps', [])}")
                print(f"   - ì‹¤íŒ¨í•œ ë‹¨ê³„: {processing_steps.get('failed_steps', [])}")
            
            data_quality = summary.get('data_quality_improvements', {})
            if data_quality:
                print(f"   - ë°ì´í„° ì™„ì „ì„±: {data_quality.get('data_completeness', 0):.1f}%")
                print(f"   - ì œê±°ëœ ê²°ì¸¡ê°’: {data_quality.get('missing_values_removed', 0)}ê°œ")
        
        # ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ ì •ë³´
        X_processed = result.get("X_processed")
        Y_processed = result.get("Y_processed")
        
        if X_processed is not None and Y_processed is not None:
            print(f"\nğŸ“ˆ ë°ì´í„°í”„ë ˆì„ ë³€í™”:")
            print(f"   - X: {X.shape} â†’ {X_processed.shape}")
            print(f"   - Y: {Y.shape} â†’ {Y_processed.shape}")
            print(f"   - X ê²°ì¸¡ê°’: {X.isnull().sum().sum()} â†’ {X_processed.isnull().sum().sum()}")
            print(f"   - Y ê²°ì¸¡ê°’: {Y.isnull().sum().sum()} â†’ {Y_processed.isnull().sum().sum()}")
            
            print(f"\nğŸ“‹ ì²˜ë¦¬ëœ X ë°ì´í„° ìƒ˜í”Œ:")
            print(X_processed.head(3))
            
            print(f"\nğŸ“‹ ì²˜ë¦¬ëœ Y ë°ì´í„° ìƒ˜í”Œ:")
            print(Y_processed.head(3))
            
            # ë°ì´í„° íƒ€ì… ì •ë³´
            print(f"\nğŸ“Š ìµœì¢… X ë°ì´í„° íƒ€ì…:")
            for col, dtype in X_processed.dtypes.items():
                print(f"   - {col}: {dtype}")
            
            print(f"\nğŸ“Š ìµœì¢… Y ë°ì´í„° íƒ€ì…:")
            for col, dtype in Y_processed.dtypes.items():
                print(f"   - {col}: {dtype}")
        
        return result
        
    except Exception as e:
        print(f"âŒ [ERROR] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}"}

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ë°ì´í„° ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # í†µí•© ë°ì´í„°ì…‹ ì‚¬ìš©
  python main.py --dataset datasets/Iris/Iris.csv
  
  # X, Y ë¶„ë¦¬ ë°ì´í„°ì…‹ ì‚¬ìš©
  python main.py --X datasets/Iris/X.csv --Y datasets/Iris/Y.csv
  
  # ì¿¼ë¦¬ì™€ í•¨ê»˜ ì‚¬ìš©
  python main.py --dataset datasets/Adult/adult.csv --query "ì†Œë“ ì˜ˆì¸¡ì„ ìœ„í•œ ì „ì²˜ë¦¬"
  python main.py --X datasets/Adult/X.csv --Y datasets/Adult/Y.csv --query "ì†Œë“ ì˜ˆì¸¡ì„ ìœ„í•œ ì „ì²˜ë¦¬"
        """
    )
    
    # ë°ì´í„°ì…‹ ì…ë ¥ ì˜µì…˜ (ìƒí˜¸ ë°°íƒ€ì )
    dataset_group = parser.add_mutually_exclusive_group(required=True)
    dataset_group.add_argument(
        "--dataset", 
        type=str,
        help="í†µí•© ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (CSV, Excel, JSON, Parquet ì§€ì›)"
    )
    
    dataset_group.add_argument(
        "--X", 
        type=str,
        help="X ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (íŠ¹ì„± ë³€ìˆ˜)"
    )
    
    dataset_group.add_argument(
        "--Y", 
        type=str,
        help="Y ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (íƒ€ê²Ÿ ë³€ìˆ˜)"
    )
    
    parser.add_argument(
        "--query", 
        type=str, 
        default=None,
        help="ì‚¬ìš©ì ì¿¼ë¦¬ (ê¸°ë³¸ê°’: ìë™ ìƒì„±)"
    )
    
    parser.add_argument(
        "--output", 
        type=str, 
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    
    args = parser.parse_args()
    
    # ì…ë ¥ ê²€ì¦
    if args.X and not args.Y:
        parser.error("--Xê°€ ì œê³µë˜ë©´ --Yë„ í•„ìš”í•©ë‹ˆë‹¤.")
    if args.Y and not args.X:
        parser.error("--Yê°€ ì œê³µë˜ë©´ --Xë„ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ì „ì—­ ë³€ìˆ˜ë¡œ dataset_path ì„¤ì • (separate_features_and_targetì—ì„œ ì‚¬ìš©)
    global dataset_path
    dataset_path = args.dataset
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = run_workflow(
        dataset_path=args.dataset,
        X_path=args.X,
        Y_path=args.Y,
        query=args.query
    )
    
    # ê²°ê³¼ ì €ì¥ (ìš”ì²­ëœ ê²½ìš°)
    if args.output and "error" not in result:
        try:
            X_processed = result.get("X_processed")
            Y_processed = result.get("Y_processed")
            
            if X_processed is not None and Y_processed is not None:
                # Xì™€ Yë¥¼ ë‹¤ì‹œ í•©ì³ì„œ ì €ì¥
                processed_df = pd.concat([X_processed, Y_processed], axis=1)
                
                output_path = args.output
                file_extension = Path(output_path).suffix.lower()
                
                if file_extension == '.csv':
                    processed_df.to_csv(output_path, index=False)
                elif file_extension in ['.xlsx', '.xls']:
                    processed_df.to_excel(output_path, index=False)
                elif file_extension == '.json':
                    processed_df.to_json(output_path, orient='records')
                elif file_extension == '.parquet':
                    processed_df.to_parquet(output_path, index=False)
                else:
                    print(f"âš ï¸  [WARNING] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ í˜•ì‹: {file_extension}")
                    processed_df.to_csv(output_path + '.csv', index=False)
                
                print(f"âœ… [SAVE] ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            print(f"âŒ [ERROR] ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 