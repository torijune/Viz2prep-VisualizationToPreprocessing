#!/usr/bin/env python3
"""
ì „ë¬¸ í”Œë˜ë„ˆ ë…¸ë“œë“¤
ê° ë„ë©”ì¸ë³„ë¡œ ì „ë¬¸í™”ëœ ì „ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ë…¸ë“œë“¤
"""

import os
import sys
import json
from typing import Dict, Any
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from workflow_state import WorkflowState


def load_preprocessing_techniques():
    """
    KBì—ì„œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    kb_path = os.path.join(os.path.dirname(__file__), "..", "KB_rag_agents", "knowledge_base", "preprocessing_codes.json")
    
    try:
        with open(kb_path, 'r', encoding='utf-8') as f:
            kb_data = json.load(f)
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ì„ ì •ë¦¬
        techniques_by_category = {}
        for category, data in kb_data.items():
            techniques = [tech['name'] for tech in data['techniques']]
            techniques_by_category[category] = {
                'description': data['description'],
                'techniques': techniques
            }
        
        return techniques_by_category
    except Exception as e:
        print(f"âš ï¸ [PLAN] KB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}


def get_available_techniques(category: str) -> str:
    """
    íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    techniques = load_preprocessing_techniques()
    
    if category in techniques:
        tech_list = techniques[category]['techniques']
        return f"=== {category.replace('_', ' ').title()} ì „ì²˜ë¦¬ ì˜µì…˜ ===\n" + \
               "\n".join([f"{i+1}. {tech}" for i, tech in enumerate(tech_list)])
    else:
        return f"âš ï¸ {category} ì¹´í…Œê³ ë¦¬ì˜ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def numeric_planner_node(state: WorkflowState) -> WorkflowState:
    """
    ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬ í”Œë˜ë„ˆ ë…¸ë“œ
    
    ğŸ” INPUT STATES:
    - query: ì‚¬ìš©ì ìš”ì²­
    - numeric_analysis: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ EDA ê²°ê³¼
    
    ğŸ“Š OUTPUT STATES:
    - numeric_plan: ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬ ê³„íš
    
    â¡ï¸ NEXT EDGE: numeric_coder
    """
    print("ğŸ“‹ [PLAN] ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘...")
    
    try:
        query = state.get("query", "")
        numeric_analysis = state.get("numeric_analysis", {})
        dataframe = state.get("dataframe")
        
        # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸°
        actual_numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()
        
        # KBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ ê°€ì ¸ì˜¤ê¸°
        available_techniques = get_available_techniques("scaling") + "\n\n" + \
                             get_available_techniques("outliers") + "\n\n" + \
                             get_available_techniques("missing_values")

        print("ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª…ë“¤: ", actual_numeric_columns)
        
        # LLMì„ ì‚¬ìš©í•˜ì—¬ ì „ë¬¸í™”ëœ ê³„íš ìˆ˜ë¦½
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=1500)
        
        prompt = f"""
ë‹¹ì‹ ì€ ìˆ˜ì¹˜í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
EDA ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì—¬ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

=== ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ===
- ì „ì²´ ì»¬ëŸ¼: {list(dataframe.columns)}
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {actual_numeric_columns}
- ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {dataframe.shape}

=== ì‚¬ìš©ì ìš”ì²­ ===
{query}

=== ìˆ˜ì¹˜í˜• EDA ë¶„ì„ ê²°ê³¼ ===
{json.dumps(numeric_analysis, indent=2, ensure_ascii=False)}

=== ë¶„ì„ ê²°ê³¼ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ===
ìœ„ EDA ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ë°ì´í„° ë¶„í¬ íŠ¹ì„± ë¶„ì„**:
   - ê° ë³€ìˆ˜ì˜ ë¶„í¬ í˜•íƒœ (ì •ê·œë¶„í¬, ì¹˜ìš°ì¹œ ë¶„í¬, ì²¨ë„ ë“±)
   - ì´ìƒì¹˜ ì¡´ì¬ ì—¬ë¶€ì™€ ì •ë„
   - ê²°ì¸¡ê°’ íŒ¨í„´

2. **ì£¼ìš” ë°œê²¬ì‚¬í•­**:
   - ê°€ì¥ ì¤‘ìš”í•œ ë°ì´í„° íŠ¹ì„±ë“¤
   - ì ì¬ì  ë¬¸ì œì ë“¤
   - ëª¨ë¸ë§ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤

3. **ì „ì²˜ë¦¬ í•„ìš”ì„± í‰ê°€**:
   - ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ì „ì²˜ë¦¬ê°€ í•„ìš”í•œì§€
   - ê° ë³€ìˆ˜ë³„ ì „ì²˜ë¦¬ ìš°ì„ ìˆœìœ„
   - ì „ì²˜ë¦¬ ë°©ë²•ì˜ ê·¼ê±°

{available_techniques}

=== ì‘ë‹µ í˜•ì‹ ===
{{
    "insights": {{
        "distribution_analysis": "ë¶„í¬ íŠ¹ì„± ë¶„ì„ ê²°ê³¼",
        "key_findings": "ì£¼ìš” ë°œê²¬ì‚¬í•­",
        "data_quality_issues": "ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë“¤",
        "preprocessing_needs": "ì „ì²˜ë¦¬ í•„ìš”ì„± í‰ê°€"
    }},
    "techniques": ["technique1", "technique2"],
    "rationale": "ê³„íš ìˆ˜ë¦½ ê·¼ê±° (ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜)",
    "priority": "high/medium/low",
    "target_columns": {actual_numeric_columns},
    "technique_details": {{
        "technique1": {{
            "columns": {actual_numeric_columns},
            "reason": "ì´ ê¸°ë²•ì„ ì„ íƒí•œ ì´ìœ ",
            "parameters": {{"param1": "value1"}}
        }}
    }}
}}

ìœ„ì—ì„œ ì œê³µëœ ì „ì²˜ë¦¬ ì˜µì…˜ ì¤‘ì—ì„œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”: {actual_numeric_columns}
"""

        response = llm.invoke([HumanMessage(content=prompt)])
        
        # JSON íŒŒì‹± ì „ì— ì‘ë‹µ ê²€ì¦
        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            content = response.content.strip()
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
        except (json.JSONDecodeError, IndexError) as e:
            print(f"âš ï¸ [PLAN] JSON íŒŒì‹± ì˜¤ë¥˜, ê¸°ë³¸ ê³„íš ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ê³„íš ìƒì„±
            result = {
                "insights": {
                    "distribution_analysis": "ê¸°ë³¸ ë¶„ì„ (JSON íŒŒì‹± ì‹¤íŒ¨)",
                    "key_findings": "ê¸°ë³¸ ë°œê²¬ì‚¬í•­",
                    "data_quality_issues": "ê¸°ë³¸ í’ˆì§ˆ ì´ìŠˆ",
                    "preprocessing_needs": "ê¸°ë³¸ ì „ì²˜ë¦¬ í•„ìš”ì„±"
                },
                "techniques": ["standard_scaling"],
                "rationale": "ê¸°ë³¸ ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬ ê³„íš (JSON íŒŒì‹± ì‹¤íŒ¨)",
                "priority": "medium",
                "target_columns": [],
                "technique_details": {}
            }
        
        numeric_plan = {
            "domain": "numeric",
            "insights": result.get("insights", {}),
            "techniques": result.get("techniques", []),
            "rationale": result.get("rationale", ""),
            "priority": result.get("priority", "medium"),
            "target_columns": result.get("target_columns", []),
            "technique_details": result.get("technique_details", {}),
            "eda_input": numeric_analysis,
            "status": "success"
        }
        
        print(f"âœ… [PLAN] ìˆ˜ì¹˜í˜• ê³„íš ì™„ë£Œ - {len(numeric_plan['techniques'])}ê°œ ê¸°ë²• ì„ íƒ")
        
        # ê³„íš ê²°ê³¼ ë¡œê¹…
        print("\n" + "="*50)
        print("ğŸ“‹ [PLAN] ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬ ê³„íš ê²°ê³¼")
        print("="*50)
        print(f"ğŸ¯ ìš°ì„ ìˆœìœ„: {numeric_plan['priority']}")
        print(f"ğŸ”§ ì„ íƒëœ ê¸°ë²•: {', '.join(numeric_plan['techniques'])}")
        if numeric_plan['target_columns']:
            print(f"ğŸ“Š ëŒ€ìƒ ë³€ìˆ˜: {', '.join(numeric_plan['target_columns'])}")
        print(f"ğŸ’¡ ê³„íš ê·¼ê±°: {numeric_plan['rationale']}")
        
        # ì¸ì‚¬ì´íŠ¸ ë¡œê¹…
        insights = numeric_plan['insights']
        if insights:
            print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
            for key, value in insights.items():
                if isinstance(value, str) and len(value) > 100:
                    print(f"   - {key}: {value[:100]}...")
                else:
                    print(f"   - {key}: {value}")
        
        return {
            **state,
            "numeric_plan": numeric_plan
        }
        
    except Exception as e:
        print(f"âŒ [PLAN] ìˆ˜ì¹˜í˜• ê³„íš ì˜¤ë¥˜: {e}")
        return {
            **state,
            "numeric_plan": {
                "domain": "numeric",
                "insights": {
                    "distribution_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                    "key_findings": "ë¶„ì„ ì‹¤íŒ¨",
                    "data_quality_issues": "ë¶„ì„ ì‹¤íŒ¨",
                    "preprocessing_needs": "ë¶„ì„ ì‹¤íŒ¨"
                },
                "techniques": ["standard_scaling"],
                "rationale": f"ê¸°ë³¸ ê³„íš (ì˜¤ë¥˜: {str(e)})",
                "priority": "medium",
                "target_columns": [],
                "technique_details": {},
                "status": "error"
            }
        }


def category_planner_node(state: WorkflowState) -> WorkflowState:
    """
    ë²”ì£¼í˜• ì „ì²˜ë¦¬ í”Œë˜ë„ˆ ë…¸ë“œ
    
    ğŸ” INPUT STATES:
    - query: ì‚¬ìš©ì ìš”ì²­
    - categorical_analysis: ë²”ì£¼í˜• ë³€ìˆ˜ EDA ê²°ê³¼
    
    ğŸ“Š OUTPUT STATES:
    - category_plan: ë²”ì£¼í˜• ì „ì²˜ë¦¬ ê³„íš
    
    â¡ï¸ NEXT EDGE: category_coder
    """
    print("ğŸ“‹ [PLAN] ë²”ì£¼í˜• ì „ì²˜ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘...")
    
    try:
        query = state.get("query", "")
        categorical_analysis = state.get("categorical_analysis", {})
        dataframe = state.get("dataframe")
        
        # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ë²”ì£¼í˜• ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸°
        actual_categorical_columns = dataframe.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # KBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ ê°€ì ¸ì˜¤ê¸°
        available_techniques = get_available_techniques("categorical_encoding") + "\n\n" + \
                             get_available_techniques("missing_values")
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=1500)
        
        prompt = f"""
ë‹¹ì‹ ì€ ë²”ì£¼í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
EDA ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì—¬ ë²”ì£¼í˜• ë³€ìˆ˜ì— íŠ¹í™”ëœ ì „ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

=== ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ===
- ì „ì²´ ì»¬ëŸ¼: {list(dataframe.columns)}
- ë²”ì£¼í˜• ì»¬ëŸ¼: {actual_categorical_columns}
- ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {dataframe.shape}

=== ì‚¬ìš©ì ìš”ì²­ ===
{query}

=== ë²”ì£¼í˜• EDA ë¶„ì„ ê²°ê³¼ ===
{json.dumps(categorical_analysis, indent=2, ensure_ascii=False)}

=== ë¶„ì„ ê²°ê³¼ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ===
ìœ„ EDA ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ë²”ì£¼í˜• ë°ì´í„° íŠ¹ì„± ë¶„ì„**:
   - ê° ë³€ìˆ˜ì˜ ì¹´ë””ë„ë¦¬í‹° (ê³ ìœ ê°’ ê°œìˆ˜)
   - ë¶„í¬ ë¶ˆê· í˜• ì •ë„
   - ê²°ì¸¡ê°’ íŒ¨í„´
   - í¬ê·€ ë²”ì£¼ ì¡´ì¬ ì—¬ë¶€

2. **ì£¼ìš” ë°œê²¬ì‚¬í•­**:
   - ê°€ì¥ ì¤‘ìš”í•œ ë²”ì£¼í˜• íŠ¹ì„±ë“¤
   - ì ì¬ì  ë¬¸ì œì ë“¤ (ë†’ì€ ì¹´ë””ë„ë¦¬í‹°, ë¶ˆê· í˜• ë“±)
   - ëª¨ë¸ë§ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤

3. **ì „ì²˜ë¦¬ í•„ìš”ì„± í‰ê°€**:
   - ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ì „ì²˜ë¦¬ê°€ í•„ìš”í•œì§€
   - ê° ë³€ìˆ˜ë³„ ì „ì²˜ë¦¬ ìš°ì„ ìˆœìœ„
   - ì „ì²˜ë¦¬ ë°©ë²•ì˜ ê·¼ê±°

{available_techniques}

=== ì‘ë‹µ í˜•ì‹ ===
{{
    "insights": {{
        "cardinality_analysis": "ì¹´ë””ë„ë¦¬í‹° ë¶„ì„ ê²°ê³¼",
        "distribution_analysis": "ë¶„í¬ íŠ¹ì„± ë¶„ì„ ê²°ê³¼",
        "key_findings": "ì£¼ìš” ë°œê²¬ì‚¬í•­",
        "data_quality_issues": "ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë“¤",
        "preprocessing_needs": "ì „ì²˜ë¦¬ í•„ìš”ì„± í‰ê°€"
    }},
    "techniques": ["technique1", "technique2"],
    "rationale": "ê³„íš ìˆ˜ë¦½ ê·¼ê±° (ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜)",
    "priority": "high/medium/low",
    "target_columns": {actual_categorical_columns},
    "technique_details": {{
        "technique1": {{
            "columns": {actual_categorical_columns},
            "reason": "ì´ ê¸°ë²•ì„ ì„ íƒí•œ ì´ìœ ",
            "parameters": {{"param1": "value1"}}
        }}
    }}
}}

ìœ„ì—ì„œ ì œê³µëœ ì „ì²˜ë¦¬ ì˜µì…˜ ì¤‘ì—ì„œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ë²”ì£¼í˜• ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”: {actual_categorical_columns}
"""

        response = llm.invoke([HumanMessage(content=prompt)])
        
        # JSON íŒŒì‹± ì „ì— ì‘ë‹µ ê²€ì¦
        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            content = response.content.strip()
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
        except (json.JSONDecodeError, IndexError) as e:
            print(f"âš ï¸ [PLAN] JSON íŒŒì‹± ì˜¤ë¥˜, ê¸°ë³¸ ê³„íš ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ê³„íš ìƒì„±
            result = {
                "insights": {
                    "cardinality_analysis": "ê¸°ë³¸ ë¶„ì„ (JSON íŒŒì‹± ì‹¤íŒ¨)",
                    "distribution_analysis": "ê¸°ë³¸ ë¶„ì„",
                    "key_findings": "ê¸°ë³¸ ë°œê²¬ì‚¬í•­",
                    "data_quality_issues": "ê¸°ë³¸ í’ˆì§ˆ ì´ìŠˆ",
                    "preprocessing_needs": "ê¸°ë³¸ ì „ì²˜ë¦¬ í•„ìš”ì„±"
                },
                "techniques": ["label_encoding"],
                "rationale": "ê¸°ë³¸ ë²”ì£¼í˜• ì „ì²˜ë¦¬ ê³„íš (JSON íŒŒì‹± ì‹¤íŒ¨)",
                "priority": "medium",
                "target_columns": [],
                "technique_details": {}
            }
        
        category_plan = {
            "domain": "category",
            "insights": result.get("insights", {}),
            "techniques": result.get("techniques", []),
            "rationale": result.get("rationale", ""),
            "priority": result.get("priority", "medium"),
            "target_columns": result.get("target_columns", []),
            "technique_details": result.get("technique_details", {}),
            "eda_input": categorical_analysis,
            "status": "success"
        }
        
        print(f"âœ… [PLAN] ë²”ì£¼í˜• ê³„íš ì™„ë£Œ - {len(category_plan['techniques'])}ê°œ ê¸°ë²• ì„ íƒ")
        
        # ê³„íš ê²°ê³¼ ë¡œê¹…
        print("\n" + "="*50)
        print("ğŸ“‹ [PLAN] ë²”ì£¼í˜• ì „ì²˜ë¦¬ ê³„íš ê²°ê³¼")
        print("="*50)
        print(f"ğŸ¯ ìš°ì„ ìˆœìœ„: {category_plan['priority']}")
        print(f"ğŸ”§ ì„ íƒëœ ê¸°ë²•: {', '.join(category_plan['techniques'])}")
        if category_plan['target_columns']:
            print(f"ğŸ“Š ëŒ€ìƒ ë³€ìˆ˜: {', '.join(category_plan['target_columns'])}")
        print(f"ğŸ’¡ ê³„íš ê·¼ê±°: {category_plan['rationale']}")
        
        # ì¸ì‚¬ì´íŠ¸ ë¡œê¹…
        insights = category_plan['insights']
        if insights:
            print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
            for key, value in insights.items():
                if isinstance(value, str) and len(value) > 100:
                    print(f"   - {key}: {value[:100]}...")
                else:
                    print(f"   - {key}: {value}")
        
        return {
            **state,
            "category_plan": category_plan
        }
        
    except Exception as e:
        print(f"âŒ [PLAN] ë²”ì£¼í˜• ê³„íš ì˜¤ë¥˜: {e}")
        return {
            **state,
            "category_plan": {
                "domain": "category",
                "insights": {
                    "cardinality_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                    "distribution_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                    "key_findings": "ë¶„ì„ ì‹¤íŒ¨",
                    "data_quality_issues": "ë¶„ì„ ì‹¤íŒ¨",
                    "preprocessing_needs": "ë¶„ì„ ì‹¤íŒ¨"
                },
                "techniques": ["label_encoding"],
                "rationale": f"ê¸°ë³¸ ê³„íš (ì˜¤ë¥˜: {str(e)})",
                "priority": "medium",
                "target_columns": [],
                "technique_details": {},
                "status": "error"
            }
        }


def outlier_planner_node(state: WorkflowState) -> WorkflowState:
    """
    ì´ìƒì¹˜ ì „ì²˜ë¦¬ í”Œë˜ë„ˆ ë…¸ë“œ
    
    ğŸ” INPUT STATES:
    - query: ì‚¬ìš©ì ìš”ì²­
    - outlier_analysis: ì´ìƒì¹˜ EDA ê²°ê³¼
    
    ğŸ“Š OUTPUT STATES:
    - outlier_plan: ì´ìƒì¹˜ ì „ì²˜ë¦¬ ê³„íš
    
    â¡ï¸ NEXT EDGE: outlier_coder
    """
    print("ğŸ“‹ [PLAN] ì´ìƒì¹˜ ì „ì²˜ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘...")
    
    try:
        query = state.get("query", "")
        outlier_analysis = state.get("outlier_analysis", {})
        dataframe = state.get("dataframe")
        
        # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸° (ì´ìƒì¹˜ëŠ” ìˆ˜ì¹˜í˜•ì—ë§Œ ì ìš©)
        actual_numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()
        
        # KBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ ê°€ì ¸ì˜¤ê¸°
        available_techniques = get_available_techniques("outliers")
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=1500)
        
        prompt = f"""
ë‹¹ì‹ ì€ ì´ìƒì¹˜ ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
EDA ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì—¬ ì´ìƒì¹˜ ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

=== ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ===
- ì „ì²´ ì»¬ëŸ¼: {list(dataframe.columns)}
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ì´ìƒì¹˜ ì²˜ë¦¬ ëŒ€ìƒ): {actual_numeric_columns}
- ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {dataframe.shape}

=== ì‚¬ìš©ì ìš”ì²­ ===
{query}

=== ì´ìƒì¹˜ EDA ë¶„ì„ ê²°ê³¼ ===
{json.dumps(outlier_analysis, indent=2, ensure_ascii=False)}

=== ë¶„ì„ ê²°ê³¼ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ===
ìœ„ EDA ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„**:
   - IQRê³¼ Z-Score ë°©ë²•ì˜ ì°¨ì´ì 
   - ê° ë³€ìˆ˜ë³„ ì´ìƒì¹˜ ë¶„í¬ íŠ¹ì„±
   - ì´ìƒì¹˜ì˜ ì‹¬ê°ë„ (ë¹„ìœ¨, ë²”ìœ„ ë“±)

2. **ì£¼ìš” ë°œê²¬ì‚¬í•­**:
   - ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ì´ìƒì¹˜ íŒ¨í„´ë“¤
   - ì´ìƒì¹˜ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì›ì¸ ì¶”ì •
   - ëª¨ë¸ë§ì— ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì˜í–¥

3. **ì²˜ë¦¬ ì „ëµ í‰ê°€**:
   - ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ì´ìƒì¹˜ ì²˜ë¦¬ê°€ í•„ìš”í•œì§€
   - ê° ë³€ìˆ˜ë³„ ì²˜ë¦¬ ë°©ë²•ì˜ ì í•©ì„±
   - ì²˜ë¦¬ ìš°ì„ ìˆœìœ„

{available_techniques}

=== ì‘ë‹µ í˜•ì‹ ===
{{
    "insights": {{
        "outlier_pattern_analysis": "ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼",
        "severity_assessment": "ì´ìƒì¹˜ ì‹¬ê°ë„ í‰ê°€",
        "key_findings": "ì£¼ìš” ë°œê²¬ì‚¬í•­",
        "potential_causes": "ì´ìƒì¹˜ ë°œìƒ ê°€ëŠ¥ ì›ì¸",
        "modeling_impact": "ëª¨ë¸ë§ì— ë¯¸ì¹  ì˜í–¥"
    }},
    "techniques": ["technique1", "technique2"],
    "rationale": "ê³„íš ìˆ˜ë¦½ ê·¼ê±° (ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜)",
    "priority": "high/medium/low",
    "target_columns": {actual_numeric_columns},
    "technique_details": {{
        "technique1": {{
            "columns": {actual_numeric_columns},
            "reason": "ì´ ê¸°ë²•ì„ ì„ íƒí•œ ì´ìœ ",
            "parameters": {{"param1": "value1"}}
        }}
    }}
}}

ìœ„ì—ì„œ ì œê³µëœ ì „ì²˜ë¦¬ ì˜µì…˜ ì¤‘ì—ì„œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”: {actual_numeric_columns}
"""

        response = llm.invoke([HumanMessage(content=prompt)])
        
        # JSON íŒŒì‹± ì „ì— ì‘ë‹µ ê²€ì¦
        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            content = response.content.strip()
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
        except (json.JSONDecodeError, IndexError) as e:
            print(f"âš ï¸ [PLAN] JSON íŒŒì‹± ì˜¤ë¥˜, ê¸°ë³¸ ê³„íš ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ê³„íš ìƒì„±
            result = {
                "insights": {
                    "outlier_pattern_analysis": "ê¸°ë³¸ ë¶„ì„ (JSON íŒŒì‹± ì‹¤íŒ¨)",
                    "severity_assessment": "ê¸°ë³¸ í‰ê°€",
                    "key_findings": "ê¸°ë³¸ ë°œê²¬ì‚¬í•­",
                    "potential_causes": "ê¸°ë³¸ ì›ì¸",
                    "modeling_impact": "ê¸°ë³¸ ì˜í–¥"
                },
                "techniques": ["iqr_outlier_detection"],
                "rationale": "ê¸°ë³¸ ì´ìƒì¹˜ ì²˜ë¦¬ ê³„íš (JSON íŒŒì‹± ì‹¤íŒ¨)",
                "priority": "medium",
                "target_columns": [],
                "technique_details": {}
            }
        
        outlier_plan = {
            "domain": "outlier",
            "insights": result.get("insights", {}),
            "techniques": result.get("techniques", []),
            "rationale": result.get("rationale", ""),
            "priority": result.get("priority", "medium"),
            "target_columns": result.get("target_columns", []),
            "technique_details": result.get("technique_details", {}),
            "eda_input": outlier_analysis,
            "status": "success"
        }
        
        print(f"âœ… [PLAN] ì´ìƒì¹˜ ê³„íš ì™„ë£Œ - {len(outlier_plan['techniques'])}ê°œ ê¸°ë²• ì„ íƒ")
        
        # ê³„íš ê²°ê³¼ ë¡œê¹…
        print("\n" + "="*50)
        print("ğŸ“‹ [PLAN] ì´ìƒì¹˜ ì „ì²˜ë¦¬ ê³„íš ê²°ê³¼")
        print("="*50)
        print(f"ğŸ¯ ìš°ì„ ìˆœìœ„: {outlier_plan['priority']}")
        print(f"ğŸ”§ ì„ íƒëœ ê¸°ë²•: {', '.join(outlier_plan['techniques'])}")
        if outlier_plan['target_columns']:
            print(f"ğŸ“Š ëŒ€ìƒ ë³€ìˆ˜: {', '.join(outlier_plan['target_columns'])}")
        print(f"ğŸ’¡ ê³„íš ê·¼ê±°: {outlier_plan['rationale']}")
        
        # ì¸ì‚¬ì´íŠ¸ ë¡œê¹…
        insights = outlier_plan['insights']
        if insights:
            print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
            for key, value in insights.items():
                if isinstance(value, str) and len(value) > 100:
                    print(f"   - {key}: {value[:100]}...")
                else:
                    print(f"   - {key}: {value}")
        
        return {
            **state,
            "outlier_plan": outlier_plan
        }
        
    except Exception as e:
        print(f"âŒ [PLAN] ì´ìƒì¹˜ ê³„íš ì˜¤ë¥˜: {e}")
        return {
            **state,
            "outlier_plan": {
                "domain": "outlier",
                "insights": {
                    "outlier_pattern_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                    "severity_assessment": "í‰ê°€ ì‹¤íŒ¨",
                    "key_findings": "ë¶„ì„ ì‹¤íŒ¨",
                    "potential_causes": "ë¶„ì„ ì‹¤íŒ¨",
                    "modeling_impact": "ë¶„ì„ ì‹¤íŒ¨"
                },
                "techniques": ["iqr_outlier_detection"],
                "rationale": f"ê¸°ë³¸ ê³„íš (ì˜¤ë¥˜: {str(e)})",
                "priority": "medium",
                "target_columns": [],
                "technique_details": {},
                "status": "error"
            }
        }


def nulldata_planner_node(state: WorkflowState) -> WorkflowState:
    """
    ê²°ì¸¡ì¹˜ ì „ì²˜ë¦¬ í”Œë˜ë„ˆ ë…¸ë“œ
    
    ğŸ” INPUT STATES:
    - query: ì‚¬ìš©ì ìš”ì²­
    - missing_duplicate_analysis: ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° EDA ê²°ê³¼
    
    ğŸ“Š OUTPUT STATES:
    - nulldata_plan: ê²°ì¸¡ì¹˜ ì „ì²˜ë¦¬ ê³„íš
    
    â¡ï¸ NEXT EDGE: nulldata_coder
    """
    print("ğŸ“‹ [PLAN] ê²°ì¸¡ì¹˜ ì „ì²˜ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘...")
    
    try:
        query = state.get("query", "")
        missing_duplicate_analysis = state.get("missing_duplicate_analysis", {})
        dataframe = state.get("dataframe")
        
        # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸°
        actual_missing_columns = dataframe.columns[dataframe.isnull().any()].tolist()
        
        # KBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ ê°€ì ¸ì˜¤ê¸°
        available_techniques = get_available_techniques("missing_values")
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=1500)
        
        prompt = f"""
ë‹¹ì‹ ì€ ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
EDA ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì—¬ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

=== ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ===
- ì „ì²´ ì»¬ëŸ¼: {list(dataframe.columns)}
- ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼: {actual_missing_columns}
- ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {dataframe.shape}
- ì´ ê²°ì¸¡ê°’ ìˆ˜: {dataframe.isnull().sum().sum()}ê°œ

=== ì‚¬ìš©ì ìš”ì²­ ===
{query}

=== ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° EDA ë¶„ì„ ê²°ê³¼ ===
{json.dumps(missing_duplicate_analysis, indent=2, ensure_ascii=False)}

=== ë¶„ì„ ê²°ê³¼ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ===
ìœ„ EDA ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„**:
   - ì „ì²´ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ê³¼ ì‹¬ê°ë„
   - ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¶„í¬ íŠ¹ì„±
   - ê²°ì¸¡ì¹˜ íŒ¨í„´ (ë¬´ì‘ìœ„ vs ì²´ê³„ì )
   - ì¤‘ë³µ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ì˜í–¥

2. **ì£¼ìš” ë°œê²¬ì‚¬í•­**:
   - ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ê²°ì¸¡ì¹˜ íŒ¨í„´ë“¤
   - ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì›ì¸ ì¶”ì •
   - ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë“¤

3. **ì²˜ë¦¬ ì „ëµ í‰ê°€**:
   - ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ê°€ í•„ìš”í•œì§€
   - ê° ë³€ìˆ˜ë³„ ì²˜ë¦¬ ë°©ë²•ì˜ ì í•©ì„±
   - ì²˜ë¦¬ ìš°ì„ ìˆœìœ„

{available_techniques}

=== ì‘ë‹µ í˜•ì‹ ===
{{
    "insights": {{
        "missing_pattern_analysis": "ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼",
        "data_quality_assessment": "ë°ì´í„° í’ˆì§ˆ í‰ê°€",
        "key_findings": "ì£¼ìš” ë°œê²¬ì‚¬í•­",
        "potential_causes": "ê²°ì¸¡ì¹˜ ë°œìƒ ê°€ëŠ¥ ì›ì¸",
        "duplicate_analysis": "ì¤‘ë³µ ë°ì´í„° ë¶„ì„"
    }},
    "techniques": ["technique1", "technique2"],
    "rationale": "ê³„íš ìˆ˜ë¦½ ê·¼ê±° (ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜)",
    "priority": "high/medium/low",
    "target_columns": {actual_missing_columns},
    "technique_details": {{
        "technique1": {{
            "columns": {actual_missing_columns},
            "reason": "ì´ ê¸°ë²•ì„ ì„ íƒí•œ ì´ìœ ",
            "parameters": {{"param1": "value1"}}
        }}
    }}
}}

ìœ„ì—ì„œ ì œê³µëœ ì „ì²˜ë¦¬ ì˜µì…˜ ì¤‘ì—ì„œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”: {list(dataframe.columns)}
"""

        response = llm.invoke([HumanMessage(content=prompt)])
        
        # JSON íŒŒì‹± ì „ì— ì‘ë‹µ ê²€ì¦
        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            content = response.content.strip()
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
        except (json.JSONDecodeError, IndexError) as e:
            print(f"âš ï¸ [PLAN] JSON íŒŒì‹± ì˜¤ë¥˜, ê¸°ë³¸ ê³„íš ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ê³„íš ìƒì„±
            result = {
                "insights": {
                    "missing_pattern_analysis": "ê¸°ë³¸ ë¶„ì„ (JSON íŒŒì‹± ì‹¤íŒ¨)",
                    "data_quality_assessment": "ê¸°ë³¸ í‰ê°€",
                    "key_findings": "ê¸°ë³¸ ë°œê²¬ì‚¬í•­",
                    "potential_causes": "ê¸°ë³¸ ì›ì¸",
                    "duplicate_analysis": "ê¸°ë³¸ ë¶„ì„"
                },
                "techniques": ["fill_numerical_median"],
                "rationale": "ê¸°ë³¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê³„íš (JSON íŒŒì‹± ì‹¤íŒ¨)",
                "priority": "high",
                "target_columns": [],
                "technique_details": {}
            }
        
        nulldata_plan = {
            "domain": "nulldata",
            "insights": result.get("insights", {}),
            "techniques": result.get("techniques", []),
            "rationale": result.get("rationale", ""),
            "priority": result.get("priority", "high"),
            "target_columns": result.get("target_columns", []),
            "technique_details": result.get("technique_details", {}),
            "eda_input": missing_duplicate_analysis,
            "status": "success"
        }
        
        print(f"âœ… [PLAN] ê²°ì¸¡ì¹˜ ê³„íš ì™„ë£Œ - {len(nulldata_plan['techniques'])}ê°œ ê¸°ë²• ì„ íƒ")
        
        # ê³„íš ê²°ê³¼ ë¡œê¹…
        print("\n" + "="*50)
        print("ğŸ“‹ [PLAN] ê²°ì¸¡ì¹˜ ì „ì²˜ë¦¬ ê³„íš ê²°ê³¼")
        print("="*50)
        print(f"ğŸ¯ ìš°ì„ ìˆœìœ„: {nulldata_plan['priority']}")
        print(f"ğŸ”§ ì„ íƒëœ ê¸°ë²•: {', '.join(nulldata_plan['techniques'])}")
        if nulldata_plan['target_columns']:
            print(f"ğŸ“Š ëŒ€ìƒ ë³€ìˆ˜: {', '.join(nulldata_plan['target_columns'])}")
        print(f"ğŸ’¡ ê³„íš ê·¼ê±°: {nulldata_plan['rationale']}")
        
        # ì¸ì‚¬ì´íŠ¸ ë¡œê¹…
        insights = nulldata_plan['insights']
        if insights:
            print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
            for key, value in insights.items():
                if isinstance(value, str) and len(value) > 100:
                    print(f"   - {key}: {value[:100]}...")
                else:
                    print(f"   - {key}: {value}")
        
        return {
            **state,
            "nulldata_plan": nulldata_plan
        }
        
    except Exception as e:
        print(f"âŒ [PLAN] ê²°ì¸¡ì¹˜ ê³„íš ì˜¤ë¥˜: {e}")
        return {
            **state,
            "nulldata_plan": {
                "domain": "nulldata",
                "insights": {
                    "missing_pattern_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                    "data_quality_assessment": "í‰ê°€ ì‹¤íŒ¨",
                    "key_findings": "ë¶„ì„ ì‹¤íŒ¨",
                    "potential_causes": "ë¶„ì„ ì‹¤íŒ¨",
                    "duplicate_analysis": "ë¶„ì„ ì‹¤íŒ¨"
                },
                "techniques": ["fill_numerical_median"],
                "rationale": f"ê¸°ë³¸ ê³„íš (ì˜¤ë¥˜: {str(e)})",
                "priority": "high",
                "target_columns": [],
                "technique_details": {},
                "status": "error"
            }
        }


def corr_planner_node(state: WorkflowState) -> WorkflowState:
    """
    ìƒê´€ê´€ê³„ ì „ì²˜ë¦¬ í”Œë˜ë„ˆ ë…¸ë“œ
    
    ğŸ” INPUT STATES:
    - query: ì‚¬ìš©ì ìš”ì²­
    - correlation_analysis: ìƒê´€ê´€ê³„ EDA ê²°ê³¼
    
    ğŸ“Š OUTPUT STATES:
    - corr_plan: ìƒê´€ê´€ê³„ ì „ì²˜ë¦¬ ê³„íš
    
    â¡ï¸ NEXT EDGE: corr_coder
    """
    print("ğŸ“‹ [PLAN] ìƒê´€ê´€ê³„ ì „ì²˜ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘...")
    
    try:
        query = state.get("query", "")
        correlation_analysis = state.get("correlation_analysis", {})
        dataframe = state.get("dataframe")
        
        # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸° (ìƒê´€ê´€ê³„ëŠ” ìˆ˜ì¹˜í˜•ì—ë§Œ ì ìš©)
        actual_numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()
        
        # KBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ ê°€ì ¸ì˜¤ê¸°
        available_techniques = get_available_techniques("feature_selection") + "\n\n" + \
                             get_available_techniques("dimensionality_reduction")
        
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, max_tokens=1500)
        
        prompt = f"""
ë‹¹ì‹ ì€ ìƒê´€ê´€ê³„ ë° íŠ¹ì„± ì„ íƒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
EDA ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì—¬ ìƒê´€ê´€ê³„ ê¸°ë°˜ ì „ì²˜ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

=== ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ì •ë³´ ===
- ì „ì²´ ì»¬ëŸ¼: {list(dataframe.columns)}
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ìƒê´€ê´€ê³„ ë¶„ì„ ëŒ€ìƒ): {actual_numeric_columns}
- ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {dataframe.shape}

=== ì‚¬ìš©ì ìš”ì²­ ===
{query}

=== ìƒê´€ê´€ê³„ EDA ë¶„ì„ ê²°ê³¼ ===
{json.dumps(correlation_analysis, indent=2, ensure_ascii=False)}

=== ë¶„ì„ ê²°ê³¼ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ===
ìœ„ EDA ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ìƒê´€ê´€ê³„ íŒ¨í„´ ë¶„ì„**:
   - ì „ì²´ ìƒê´€ê´€ê³„ ë¶„í¬ íŠ¹ì„±
   - ê°•í•œ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒë“¤
   - ë‹¤ì¤‘ê³µì„ ì„± ì˜ì‹¬ ë³€ìˆ˜ë“¤
   - íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„

2. **ì£¼ìš” ë°œê²¬ì‚¬í•­**:
   - ê°€ì¥ ì¤‘ìš”í•œ ìƒê´€ê´€ê³„ íŒ¨í„´ë“¤
   - ì ì¬ì  ë¬¸ì œì ë“¤ (ë‹¤ì¤‘ê³µì„ ì„±, ì•½í•œ ìƒê´€ê´€ê³„ ë“±)
   - ëª¨ë¸ë§ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤

3. **íŠ¹ì„± ì„ íƒ ì „ëµ í‰ê°€**:
   - ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ì œê±°/ì„ íƒë˜ì–´ì•¼ í•˜ëŠ”ì§€
   - ê° ë³€ìˆ˜ë³„ ì„ íƒ/ì œê±° ê·¼ê±°
   - íŠ¹ì„± ì„ íƒ ìš°ì„ ìˆœìœ„

{available_techniques}

=== ì‘ë‹µ í˜•ì‹ ===
{{
    "insights": {{
        "correlation_pattern_analysis": "ìƒê´€ê´€ê³„ íŒ¨í„´ ë¶„ì„ ê²°ê³¼",
        "multicollinearity_assessment": "ë‹¤ì¤‘ê³µì„ ì„± í‰ê°€",
        "key_findings": "ì£¼ìš” ë°œê²¬ì‚¬í•­",
        "feature_importance": "íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„",
        "modeling_impact": "ëª¨ë¸ë§ì— ë¯¸ì¹  ì˜í–¥"
    }},
    "techniques": ["technique1", "technique2"],
    "rationale": "ê³„íš ìˆ˜ë¦½ ê·¼ê±° (ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜)",
    "priority": "high/medium/low",
    "target_columns": {actual_numeric_columns},
    "technique_details": {{
        "technique1": {{
            "columns": {actual_numeric_columns},
            "reason": "ì´ ê¸°ë²•ì„ ì„ íƒí•œ ì´ìœ ",
            "parameters": {{"param1": "value1"}}
        }}
    }}
}}

ìœ„ì—ì„œ ì œê³µëœ ì „ì²˜ë¦¬ ì˜µì…˜ ì¤‘ì—ì„œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”: {actual_numeric_columns}
"""

        response = llm.invoke([HumanMessage(content=prompt)])
        
        # JSON íŒŒì‹± ì „ì— ì‘ë‹µ ê²€ì¦
        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            content = response.content.strip()
            if content.startswith('```json'):
                content = content.split('```json')[1].split('```')[0].strip()
            elif content.startswith('```'):
                content = content.split('```')[1].split('```')[0].strip()
            
            result = json.loads(content)
        except (json.JSONDecodeError, IndexError) as e:
            print(f"âš ï¸ [PLAN] JSON íŒŒì‹± ì˜¤ë¥˜, ê¸°ë³¸ ê³„íš ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ê³„íš ìƒì„±
            result = {
                "insights": {
                    "correlation_pattern_analysis": "ê¸°ë³¸ ë¶„ì„ (JSON íŒŒì‹± ì‹¤íŒ¨)",
                    "multicollinearity_assessment": "ê¸°ë³¸ í‰ê°€",
                    "key_findings": "ê¸°ë³¸ ë°œê²¬ì‚¬í•­",
                    "feature_importance": "ê¸°ë³¸ ë¶„ì„",
                    "modeling_impact": "ê¸°ë³¸ ì˜í–¥"
                },
                "techniques": ["correlation_filter"],
                "rationale": "ê¸°ë³¸ ìƒê´€ê´€ê³„ ì²˜ë¦¬ ê³„íš (JSON íŒŒì‹± ì‹¤íŒ¨)",
                "priority": "low",
                "target_columns": [],
                "technique_details": {}
            }
        
        corr_plan = {
            "domain": "correlation",
            "insights": result.get("insights", {}),
            "techniques": result.get("techniques", []),
            "rationale": result.get("rationale", ""),
            "priority": result.get("priority", "low"),  # ìƒê´€ê´€ê³„ ì²˜ë¦¬ëŠ” ë³´í†µ ë‚®ì€ ìš°ì„ ìˆœìœ„
            "target_columns": result.get("target_columns", []),
            "technique_details": result.get("technique_details", {}),
            "eda_input": correlation_analysis,
            "status": "success"
        }
        
        print(f"âœ… [PLAN] ìƒê´€ê´€ê³„ ê³„íš ì™„ë£Œ - {len(corr_plan['techniques'])}ê°œ ê¸°ë²• ì„ íƒ")
        
        # ê³„íš ê²°ê³¼ ë¡œê¹…
        print("\n" + "="*50)
        print("ğŸ“‹ [PLAN] ìƒê´€ê´€ê³„ ì „ì²˜ë¦¬ ê³„íš ê²°ê³¼")
        print("="*50)
        print(f"ğŸ¯ ìš°ì„ ìˆœìœ„: {corr_plan['priority']}")
        print(f"ğŸ”§ ì„ íƒëœ ê¸°ë²•: {', '.join(corr_plan['techniques'])}")
        if corr_plan['target_columns']:
            print(f"ğŸ“Š ëŒ€ìƒ ë³€ìˆ˜: {', '.join(corr_plan['target_columns'])}")
        print(f"ğŸ’¡ ê³„íš ê·¼ê±°: {corr_plan['rationale']}")
        
        # ì¸ì‚¬ì´íŠ¸ ë¡œê¹…
        insights = corr_plan['insights']
        if insights:
            print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
            for key, value in insights.items():
                if isinstance(value, str) and len(value) > 100:
                    print(f"   - {key}: {value[:100]}...")
                else:
                    print(f"   - {key}: {value}")
        
        return {
            **state,
            "corr_plan": corr_plan
        }
        
    except Exception as e:
        print(f"âŒ [PLAN] ìƒê´€ê´€ê³„ ê³„íš ì˜¤ë¥˜: {e}")
        return {
            **state,
            "corr_plan": {
                "domain": "correlation",
                "insights": {
                    "correlation_pattern_analysis": "ë¶„ì„ ì‹¤íŒ¨",
                    "multicollinearity_assessment": "í‰ê°€ ì‹¤íŒ¨",
                    "key_findings": "ë¶„ì„ ì‹¤íŒ¨",
                    "feature_importance": "ë¶„ì„ ì‹¤íŒ¨",
                    "modeling_impact": "ë¶„ì„ ì‹¤íŒ¨"
                },
                "techniques": ["correlation_filter"],
                "rationale": f"ê¸°ë³¸ ê³„íš (ì˜¤ë¥˜: {str(e)})",
                "priority": "low",
                "target_columns": [],
                "technique_details": {},
                "status": "error"
            }
        } 