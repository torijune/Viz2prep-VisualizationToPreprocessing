"""
ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ë¶„ì„ í…ìŠ¤íŠ¸ ì—ì´ì „íŠ¸
isnull, missing_data_percentage, duplicated ë“±ì„ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
from typing import Dict, Any
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage


def analyze_missing_and_duplicate_data(inputs: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê²°ì¸¡ì¹˜ì™€ ì¤‘ë³µ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    Args:
        inputs: DataFrameì´ í¬í•¨ëœ ì…ë ¥ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ë¶„ì„ ê²°ê³¼ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    df = inputs["dataframe"]
    
    # ë¶„ì„ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    analysis_text = "=== ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ë¶„ì„ ===\n\n"
    
    # 1. ê²°ì¸¡ì¹˜ ë¶„ì„
    analysis_text += "ğŸ“Š ê²°ì¸¡ì¹˜ ë¶„ì„:\n"
    
    # ì „ì²´ ê²°ì¸¡ì¹˜ ê°œìˆ˜
    total_missing = df.isnull().sum().sum()
    total_cells = df.size
    total_missing_percentage = (total_missing / total_cells) * 100
    
    analysis_text += f"   - ì „ì²´ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {total_missing}ê°œ\n"
    analysis_text += f"   - ì „ì²´ ë°ì´í„° ì…€ ìˆ˜: {total_cells}ê°œ\n"
    analysis_text += f"   - ì „ì²´ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {total_missing_percentage:.2f}%\n\n"
    
    # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¶„ì„
    missing_by_column = df.isnull().sum()
    missing_percentage_by_column = (missing_by_column / len(df)) * 100
    
    analysis_text += "   ğŸ“‹ ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¶„ì„:\n"
    for col in df.columns:
        missing_count = missing_by_column[col]
        missing_percentage = missing_percentage_by_column[col]
        
        if missing_count > 0:
            analysis_text += f"     - {col}: {missing_count}ê°œ ({missing_percentage:.2f}%)\n"
        else:
            analysis_text += f"     - {col}: ê²°ì¸¡ì¹˜ ì—†ìŒ\n"
    
    # ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„
    analysis_text += "\n   ğŸ” ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„:\n"
    
    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ë“¤
    rows_with_missing = df.isnull().any(axis=1).sum()
    rows_with_missing_percentage = (rows_with_missing / len(df)) * 100
    
    analysis_text += f"     - ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰: {rows_with_missing}ê°œ ({rows_with_missing_percentage:.2f}%)\n"
    analysis_text += f"     - ì™„ì „í•œ í–‰: {len(df) - rows_with_missing}ê°œ\n"
    
    # ê²°ì¸¡ì¹˜ íŒ¨í„´ (ì—¬ëŸ¬ ì»¬ëŸ¼ì—ì„œ ë™ì‹œì— ê²°ì¸¡)
    missing_patterns = df.isnull().sum(axis=1).value_counts().sort_index()
    analysis_text += "     - ê²°ì¸¡ì¹˜ ê°œìˆ˜ë³„ í–‰ ë¶„í¬:\n"
    for missing_count, row_count in missing_patterns.items():
        if missing_count > 0:
            percentage = (row_count / len(df)) * 100
            analysis_text += f"       * {missing_count}ê°œ ê²°ì¸¡: {row_count}í–‰ ({percentage:.2f}%)\n"
    
    # 2. ì¤‘ë³µ ë°ì´í„° ë¶„ì„
    analysis_text += "\nğŸ“Š ì¤‘ë³µ ë°ì´í„° ë¶„ì„:\n"
    
    # ì „ì²´ ì¤‘ë³µ í–‰
    total_duplicates = df.duplicated().sum()
    total_duplicates_percentage = (total_duplicates / len(df)) * 100
    
    analysis_text += f"   - ì „ì²´ ì¤‘ë³µ í–‰: {total_duplicates}ê°œ ({total_duplicates_percentage:.2f}%)\n"
    
    # ì¤‘ë³µ íŒ¨í„´ ë¶„ì„
    if total_duplicates > 0:
        duplicate_counts = df.duplicated(keep=False).sum()
        analysis_text += f"   - ì¤‘ë³µ íŒ¨í„´ì´ ìˆëŠ” í–‰: {duplicate_counts}ê°œ\n"
        
        # ì¤‘ë³µ ê·¸ë£¹ ë¶„ì„
        duplicate_groups = df[df.duplicated(keep=False)].groupby(df.columns.tolist()).size()
        analysis_text += f"   - ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {len(duplicate_groups)}ê°œ\n"
        
        # ê°€ì¥ ë§ì´ ì¤‘ë³µëœ íŒ¨í„´
        most_common_duplicate = duplicate_groups.idxmax()
        most_common_count = duplicate_groups.max()
        analysis_text += f"   - ê°€ì¥ ë§ì´ ì¤‘ë³µëœ íŒ¨í„´: {most_common_count}ë²ˆ\n"
    
    # 3. ë°ì´í„° í’ˆì§ˆ í‰ê°€
    analysis_text += "\nğŸ“Š ë°ì´í„° í’ˆì§ˆ í‰ê°€:\n"
    
    # ê²°ì¸¡ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í’ˆì§ˆ í‰ê°€
    if total_missing_percentage < 5:
        missing_quality = "ìš°ìˆ˜"
    elif total_missing_percentage < 15:
        missing_quality = "ì–‘í˜¸"
    elif total_missing_percentage < 30:
        missing_quality = "ë³´í†µ"
    else:
        missing_quality = "ë¶ˆëŸ‰"
    
    analysis_text += f"   - ê²°ì¸¡ì¹˜ í’ˆì§ˆ: {missing_quality} ({total_missing_percentage:.2f}%)\n"
    
    # ì¤‘ë³µ ê¸°ì¤€ìœ¼ë¡œ í’ˆì§ˆ í‰ê°€
    if total_duplicates_percentage < 1:
        duplicate_quality = "ìš°ìˆ˜"
    elif total_duplicates_percentage < 5:
        duplicate_quality = "ì–‘í˜¸"
    elif total_duplicates_percentage < 15:
        duplicate_quality = "ë³´í†µ"
    else:
        duplicate_quality = "ë¶ˆëŸ‰"
    
    analysis_text += f"   - ì¤‘ë³µ ë°ì´í„° í’ˆì§ˆ: {duplicate_quality} ({total_duplicates_percentage:.2f}%)\n"
    
    # 4. ì „ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­
    analysis_text += "\nğŸ“‹ ì „ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­:\n"
    
    # ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì»¬ëŸ¼ë“¤
    high_missing_columns = missing_percentage_by_column[missing_percentage_by_column > 50].index.tolist()
    if high_missing_columns:
        analysis_text += f"   - ê²°ì¸¡ì¹˜ê°€ 50% ì´ìƒì¸ ì»¬ëŸ¼: {', '.join(high_missing_columns)}\n"
        analysis_text += "     â†’ ì œê±° ê³ ë ¤ ë˜ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ í•„ìš”\n"
    
    # ê²°ì¸¡ì¹˜ê°€ ì ì€ ì»¬ëŸ¼ë“¤
    low_missing_columns = missing_percentage_by_column[(missing_percentage_by_column > 0) & (missing_percentage_by_column <= 10)].index.tolist()
    if low_missing_columns:
        analysis_text += f"   - ê²°ì¸¡ì¹˜ê°€ 10% ì´í•˜ì¸ ì»¬ëŸ¼: {', '.join(low_missing_columns)}\n"
        analysis_text += "     â†’ í‰ê· /ì¤‘ì•™ê°’/ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥\n"
    
    # ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬
    if total_duplicates > 0:
        analysis_text += f"   - ì¤‘ë³µ ë°ì´í„°: {total_duplicates}ê°œ ì œê±° ê¶Œì¥\n"
    
    try:
        # LLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,
            max_tokens=1000
        )
        
        llm_prompt = f"""
        ë‹¤ìŒì€ ë°ì´í„°ì…‹ì˜ ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

        {analysis_text}

        ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

        1. **ì£¼ìš” ë°œê²¬ì‚¬í•­**: ê°€ì¥ ì¤‘ìš”í•œ ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë“¤ì„ ìš”ì•½
        2. **ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„**: ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•˜ëŠ” íŒ¨í„´ê³¼ ì›ì¸ ì¶”ì •
        3. **ì¤‘ë³µ ë°ì´í„° ë¶„ì„**: ì¤‘ë³µ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ì˜ë¯¸
        4. **ì „ì²˜ë¦¬ ì „ëµ**: êµ¬ì²´ì ì¸ ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬ ë°©ë²• ì œì•ˆ

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

        ## ì£¼ìš” ë°œê²¬ì‚¬í•­
        [ê°€ì¥ ì¤‘ìš”í•œ ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë“¤]

        ## ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„
        [ê²°ì¸¡ì¹˜ ë°œìƒ íŒ¨í„´ê³¼ ì›ì¸ ì¶”ì •]

        ## ì¤‘ë³µ ë°ì´í„° ë¶„ì„
        [ì¤‘ë³µ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ì˜ë¯¸]

        ## ì „ì²˜ë¦¬ ì „ëµ
        [êµ¬ì²´ì ì¸ ì²˜ë¦¬ ë°©ë²• ì œì•ˆ]
        """
        
        response = llm.invoke([HumanMessage(content=llm_prompt)])
        llm_insights = response.content
        
        analysis_text += f"\n\n=== LLM ì¶”ê°€ ì¸ì‚¬ì´íŠ¸ ===\n{llm_insights}"
        
    except Exception as e:
        print(f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
    
    return {
        **inputs,
        "missing_duplicate_analysis": analysis_text
    }


# LangGraph ë…¸ë“œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
missing_duplicate_text_agent = RunnableLambda(analyze_missing_and_duplicate_data) 